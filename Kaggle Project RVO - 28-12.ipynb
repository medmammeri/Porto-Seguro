{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrique de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Indice de gini normalisé comme métrique pour mesurer la précision\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def gini(y, y_pred_prob):\n",
    "    fpr, tpr, thr = roc_curve(y, y_pred_prob, pos_label=1)\n",
    "    g = 2 * auc(fpr, tpr) - 1\n",
    "    return g\n",
    "\n",
    "    \n",
    "def normalized_gini(y, y_pred_prob):\n",
    "    return gini(y, y_pred_prob) / gini(y, y)\n",
    "\n",
    "\n",
    "norm_gini = make_scorer(normalized_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)\n",
    "\n",
    "def normalized_gini(y, pred):\n",
    "    return gini(y, pred) / gini(y, y)\n",
    "\n",
    "# custom normalized gini score for xgb model\n",
    "def gini_xgb(y, pred):\n",
    "    y = y.get_label()\n",
    "    return 'gini', normalized_gini(y, pred)\n",
    "\n",
    "norm_gini = make_scorer(normalized_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.drop('target',axis = 1)\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    573518\n",
       "1     21694\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a beaucoup plus de valeurs 0 que de 1, donc on s'attend à ce que les prédictions de proba pour la classe 1 soient faibles\n",
    "et on va privilégier donc l'utilisation de la métrique roc_auc_score vu que le nombre de 0 et de 1 n'est pas équilibré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03',\n",
       "       'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin',\n",
       "       'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
       "       'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15',\n",
       "       'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01',\n",
       "       'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat', 'ps_car_02_cat',\n",
       "       'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat',\n",
       "       'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat',\n",
       "       'ps_car_11_cat', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14',\n",
       "       'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04',\n",
       "       'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09',\n",
       "       'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14',\n",
       "       'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin',\n",
       "       'ps_calc_19_bin', 'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "[      7       9      13 ..., 1488017 1488021 1488027]\n",
      "target\n",
      "[0 1]\n",
      "ps_ind_01\n",
      "[0 1 2 3 4 5 6 7]\n",
      "ps_ind_02_cat\n",
      "[-1  1  2  3  4]\n",
      "ps_ind_03\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "ps_ind_04_cat\n",
      "[-1  0  1]\n",
      "ps_ind_05_cat\n",
      "[-1  0  1  2  3  4  5  6]\n",
      "ps_ind_06_bin\n",
      "[0 1]\n",
      "ps_ind_07_bin\n",
      "[0 1]\n",
      "ps_ind_08_bin\n",
      "[0 1]\n",
      "ps_ind_09_bin\n",
      "[0 1]\n",
      "ps_ind_10_bin\n",
      "[0 1]\n",
      "ps_ind_11_bin\n",
      "[0 1]\n",
      "ps_ind_12_bin\n",
      "[0 1]\n",
      "ps_ind_13_bin\n",
      "[0 1]\n",
      "ps_ind_14\n",
      "[0 1 2 3 4]\n",
      "ps_ind_15\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "ps_ind_16_bin\n",
      "[0 1]\n",
      "ps_ind_17_bin\n",
      "[0 1]\n",
      "ps_ind_18_bin\n",
      "[0 1]\n",
      "ps_reg_01\n",
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n",
      "ps_reg_02\n",
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.   1.1  1.2  1.3  1.4\n",
      "  1.5  1.6  1.7  1.8]\n",
      "ps_reg_03\n",
      "[-1.          0.06123724  0.075      ...,  3.49079146  3.78772689\n",
      "  4.03794502]\n",
      "ps_car_01_cat\n",
      "[-1  0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "ps_car_02_cat\n",
      "[-1  0  1]\n",
      "ps_car_03_cat\n",
      "[-1  0  1]\n",
      "ps_car_04_cat\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "ps_car_05_cat\n",
      "[-1  0  1]\n",
      "ps_car_06_cat\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "ps_car_07_cat\n",
      "[-1  0  1]\n",
      "ps_car_08_cat\n",
      "[0 1]\n",
      "ps_car_09_cat\n",
      "[-1  0  1  2  3  4]\n",
      "ps_car_10_cat\n",
      "[0 1 2]\n",
      "ps_car_11_cat\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "ps_car_11\n",
      "[-1  0  1  2  3]\n",
      "ps_car_12\n",
      "[-1.          0.1         0.14142136  0.14832397  0.17320508  0.28284271\n",
      "  0.3088689   0.31527766  0.31559468  0.31575307  0.31606961  0.31622777\n",
      "  0.33166248  0.34641016  0.35199432  0.3524202   0.36        0.36041643\n",
      "  0.36055513  0.36878178  0.37040518  0.37255872  0.3726929   0.37416574\n",
      "  0.38639358  0.38691084  0.38729833  0.39433488  0.39471509  0.39522146\n",
      "  0.39560081  0.39724048  0.39749214  0.39761791  0.39799497  0.39812058\n",
      "  0.39837169  0.39862263  0.39874804  0.39899875  0.39937451  0.39949969\n",
      "  0.39962482  0.39974992  0.39987498  0.4         0.40841156  0.41231056\n",
      "  0.41797129  0.41964271  0.42178193  0.42201896  0.42355637  0.4236744\n",
      "  0.4237924   0.4240283   0.4241462   0.42426407  0.42825226  0.43566042\n",
      "  0.43588989  0.44384682  0.44418465  0.44542115  0.44553339  0.44586994\n",
      "  0.44598206  0.44620623  0.44654227  0.44665423  0.44676616  0.44687806\n",
      "  0.44698993  0.44710178  0.4472136   0.46368092  0.46432747  0.46518813\n",
      "  0.46529561  0.46593991  0.46882833  0.46904158  0.4747631   0.47539457\n",
      "  0.47822589  0.47906158  0.47958315  0.48        0.48487112  0.48518038\n",
      "  0.48969378  0.48989795  0.49457052  0.49568135  0.49638695  0.49769469\n",
      "  0.49929951  0.49939964  0.49949975  0.49969991  0.49979996  0.5\n",
      "  0.50049975  0.50990195  0.5105879   0.51536395  0.51961524  0.52640289\n",
      "  0.52763624  0.52810984  0.52848841  0.52896125  0.52905576  0.52915026\n",
      "  0.53244718  0.53786615  0.53851648  0.54378304  0.54424259  0.54470175\n",
      "  0.54516053  0.54580216  0.54607692  0.54644304  0.54662601  0.54726593\n",
      "  0.54735729  0.54744863  0.54772256  0.56249444  0.56559703  0.56568542\n",
      "  0.57445626  0.5745433   0.58120564  0.58309519  0.59143892  0.59160798\n",
      "  0.6         0.60506198  0.60827625  0.61465437  0.61571097  0.6164414\n",
      "  0.6244998   0.62498     0.62609903  0.62912638  0.63087241  0.63213923\n",
      "  0.63245553  0.63976558  0.64007812  0.64031242  0.64591021  0.6476882\n",
      "  0.64776539  0.64791975  0.64799691  0.64807407  0.65559134  0.6556676\n",
      "  0.65574385  0.66317419  0.66332496  0.67082039  0.678233    0.68563839\n",
      "  0.69282032  0.69942834  0.70092796  0.70469852  0.70519501  0.70710678\n",
      "  0.72111026  0.72318739  0.74161985  0.75498344  0.76811457  0.77278716\n",
      "  0.78102497  0.78740079  0.81853528  1.26491106]\n",
      "ps_car_13\n",
      "[ 0.25061907  0.28733601  0.29041151 ...,  3.44256295  3.51580258  3.720626  ]\n",
      "ps_car_14\n",
      "[-1.          0.10954451  0.1183216   0.13601471  0.2         0.2078461\n",
      "  0.20976177  0.21095023  0.21307276  0.21794495  0.22135944  0.22315914\n",
      "  0.2236068   0.22427661  0.23021729  0.23323808  0.23345235  0.23452079\n",
      "  0.24899799  0.26172505  0.26457513  0.27386128  0.27748874  0.2792848\n",
      "  0.28195744  0.28284271  0.28372522  0.28548205  0.28600699  0.28635642\n",
      "  0.28722813  0.28809721  0.28827071  0.28879058  0.28982753  0.29257478\n",
      "  0.29325757  0.29410882  0.29444864  0.29495762  0.29512709  0.29529646\n",
      "  0.29614186  0.29664794  0.29732137  0.29832868  0.29916551  0.3\n",
      "  0.30033315  0.30066593  0.30083218  0.30166206  0.30199338  0.30248967\n",
      "  0.30331502  0.30413813  0.30495901  0.30561414  0.3057777   0.30610456\n",
      "  0.30659419  0.30708305  0.30740852  0.30773365  0.30805844  0.3082207\n",
      "  0.30854497  0.30870698  0.30903074  0.30967725  0.31032241  0.31064449\n",
      "  0.31112698  0.31144823  0.31256999  0.31304952  0.3132092   0.31352831\n",
      "  0.3138471   0.31400637  0.3144837   0.31464265  0.31527766  0.31622777\n",
      "  0.31654384  0.31670175  0.31701735  0.31733263  0.31749016  0.3176476\n",
      "  0.31780497  0.31811947  0.31827661  0.31859065  0.31906112  0.31921779\n",
      "  0.31937439  0.31953091  0.31968735  0.31984371  0.32        0.32015621\n",
      "  0.32046841  0.32062439  0.3207803   0.32093613  0.32109189  0.32124757\n",
      "  0.32140317  0.3215587   0.32171416  0.32186954  0.32249031  0.32264532\n",
      "  0.3232646   0.32403703  0.3241913   0.32434549  0.32465366  0.32480764\n",
      "  0.32496154  0.32557641  0.32572995  0.32603681  0.32619013  0.32634338\n",
      "  0.32649655  0.32664966  0.32680269  0.32695565  0.32710854  0.32726136\n",
      "  0.32741411  0.32756679  0.32787193  0.32802439  0.32817678  0.32848135\n",
      "  0.32863353  0.32878564  0.32893768  0.32924155  0.32939338  0.32969683\n",
      "  0.32984845  0.33        0.33015148  0.33030289  0.33045423  0.33060551\n",
      "  0.33075671  0.33090784  0.33136083  0.33151169  0.33166248  0.33211444\n",
      "  0.33226495  0.3324154   0.33316662  0.33361655  0.33376639  0.33391616\n",
      "  0.33406586  0.3342155   0.33466401  0.33481338  0.33496268  0.3354102\n",
      "  0.33555923  0.33570821  0.33585711  0.33600595  0.33615473  0.33689761\n",
      "  0.33734256  0.33749074  0.33763886  0.33778692  0.33793491  0.33808283\n",
      "  0.33837849  0.33867388  0.33882149  0.3391165   0.33955854  0.33985291\n",
      "  0.34044089  0.34058773  0.34088121  0.34117444  0.34132096  0.34146742\n",
      "  0.34176015  0.34205263  0.34219877  0.34234486  0.34249088  0.34278273\n",
      "  0.34292856  0.34351128  0.34365681  0.34394767  0.34409301  0.34423829\n",
      "  0.34438351  0.34452866  0.34467376  0.34496377  0.34539832  0.34554305\n",
      "  0.34568772  0.34583233  0.34597688  0.34626579  0.34641016  0.34669872\n",
      "  0.3468429   0.3471311   0.34727511  0.34741906  0.34785054  0.34799425\n",
      "  0.3481379   0.3485685   0.34871192  0.34885527  0.34899857  0.34928498\n",
      "  0.34985711  0.35        0.35014283  0.35042831  0.35071356  0.3508561\n",
      "  0.35099858  0.35142567  0.35171011  0.35185224  0.35213634  0.35284558\n",
      "  0.35312887  0.35341194  0.35355339  0.35425979  0.35454196  0.35468296\n",
      "  0.3548239   0.35496479  0.35566838  0.35608988  0.35623026  0.35637059\n",
      "  0.35679126  0.35707142  0.35721142  0.35749126  0.35777088  0.35818989\n",
      "  0.35832946  0.35846897  0.35860842  0.35888717  0.3591657   0.35944402\n",
      "  0.35958309  0.35972212  0.35986108  0.36        0.36013886  0.36055513\n",
      "  0.36083237  0.36097091  0.36124784  0.36138622  0.36152455  0.36166283\n",
      "  0.36180105  0.36193922  0.36221541  0.36262929  0.36290495  0.3630427\n",
      "  0.36331804  0.36373067  0.36386811  0.36400549  0.36414283  0.36469165\n",
      "  0.36496575  0.36510273  0.36537652  0.36551334  0.36565011  0.36578682\n",
      "  0.36592349  0.3660601   0.36633318  0.36646964  0.36660606  0.36674242\n",
      "  0.36742346  0.36769553  0.36783148  0.36810325  0.36837481  0.36851052\n",
      "  0.36878178  0.3691883   0.36959437  0.36986484  0.37        0.37013511\n",
      "  0.37027017  0.37040518  0.37067506  0.37080992  0.37107951  0.37134889\n",
      "  0.37148351  0.37161808  0.3720215   0.37215588  0.37242449  0.37255872\n",
      "  0.3726929   0.37282704  0.37296112  0.37309516  0.37349699  0.37363083\n",
      "  0.37403208  0.37416574  0.37429935  0.3744329   0.37456642  0.37469988\n",
      "  0.3748333   0.37536649  0.37549967  0.37603191  0.37616486  0.3764306\n",
      "  0.37656341  0.37682887  0.37735925  0.37749172  0.37802116  0.37815341\n",
      "  0.37841776  0.37854986  0.37881394  0.37894591  0.37934153  0.37947332\n",
      "  0.37960506  0.3798684   0.38013156  0.38026307  0.38078866  0.38091994\n",
      "  0.38131352  0.38144462  0.38157568  0.38170669  0.38196859  0.38209946\n",
      "  0.38236109  0.38249183  0.38262253  0.38275318  0.38327536  0.38340579\n",
      "  0.38353618  0.38366652  0.38392708  0.38405729  0.38418745  0.38457769\n",
      "  0.38470768  0.38483763  0.38496753  0.38509739  0.38522721  0.38535698\n",
      "  0.38548671  0.38561639  0.38587563  0.38600518  0.38613469  0.3866523\n",
      "  0.38678159  0.38691084  0.38704005  0.38729833  0.38742741  0.38794329\n",
      "  0.38832976  0.38845849  0.38858718  0.38871583  0.38884444  0.38897301\n",
      "  0.38923001  0.38935845  0.38961519  0.38974351  0.38987177  0.39\n",
      "  0.39012818  0.39025633  0.39038443  0.39051248  0.3906405   0.39076847\n",
      "  0.39089641  0.3910243   0.39115214  0.39140772  0.39179076  0.39204592\n",
      "  0.39242834  0.39268308  0.39293765  0.39306488  0.39319206  0.39331921\n",
      "  0.39344631  0.39370039  0.39382737  0.39408121  0.39433488  0.39496835\n",
      "  0.39522146  0.39534795  0.39560081  0.39572718  0.39585351  0.3959798\n",
      "  0.39623226  0.39635842  0.39648455  0.3968627   0.39698866  0.39724048\n",
      "  0.39749214  0.39761791  0.39774364  0.39812058  0.39837169  0.39849718\n",
      "  0.39862263  0.39874804  0.39887341  0.39912404  0.3992493   0.39937451\n",
      "  0.39962482  0.39987498  0.4         0.40037482  0.40049969  0.40062451\n",
      "  0.40087405  0.40099875  0.40124805  0.40137264  0.40162171  0.40174619\n",
      "  0.40187063  0.40224371  0.40236799  0.40249224  0.40261644  0.40274061\n",
      "  0.40311289  0.40348482  0.40373258  0.40385641  0.4039802   0.40435133\n",
      "  0.40459857  0.40472213  0.40484565  0.40496913  0.40521599  0.40533936\n",
      "  0.405586    0.40570926  0.40620192  0.40632499  0.40669399  0.40681691\n",
      "  0.40743098  0.40804412  0.40841156  0.40865633  0.40914545  0.40926764\n",
      "  0.40938979  0.40975603  0.40987803  0.41012193  0.41024383  0.41036569\n",
      "  0.41048752  0.41073106  0.4110961   0.41146081  0.41170378  0.41218928\n",
      "  0.41231056  0.41243181  0.41255303  0.41291646  0.41303753  0.41352146\n",
      "  0.41412558  0.41448764  0.41460825  0.41472883  0.41509035  0.41533119\n",
      "  0.41545156  0.41581246  0.41593269  0.41653331  0.41665333  0.41677332\n",
      "  0.41713307  0.41749251  0.41761226  0.41773197  0.41785165  0.41797129\n",
      "  0.4180909   0.41821047  0.41833001  0.41856899  0.4189272   0.41904654\n",
      "  0.41916584  0.41928511  0.41952354  0.41964271  0.42011903  0.42071368\n",
      "  0.42083251  0.42107007  0.42118879  0.42130749  0.42154478  0.42166337\n",
      "  0.42190046  0.4224926   0.42284749  0.42308392  0.42320208  0.42332021\n",
      "  0.42343831  0.4236744   0.4237924   0.4241462   0.42426407  0.42449971\n",
      "  0.42461747  0.42473521  0.42485292  0.42520583  0.42532341  0.42544095\n",
      "  0.42567593  0.42602817  0.42661458  0.42684892  0.42696604  0.42720019\n",
      "  0.42731721  0.42743421  0.42778499  0.42790186  0.428369    0.42860238\n",
      "  0.42895221  0.42918527  0.42930176  0.42953463  0.42976738  0.43\n",
      "  0.43011626  0.4302325   0.43046486  0.430581    0.43069711  0.43081318\n",
      "  0.43116122  0.43127717  0.43162484  0.43185646  0.43197222  0.43208795\n",
      "  0.43243497  0.4327817   0.43289722  0.4330127   0.43358967  0.43393548\n",
      "  0.43416587  0.4347413   0.4348563   0.43497126  0.43531598  0.43543082\n",
      "  0.43588989  0.43634848  0.43646306  0.43669211  0.43703547  0.43749286\n",
      "  0.43772137  0.43817805  0.43829214  0.4384062   0.43852024  0.43874822\n",
      "  0.43943145  0.43954522  0.43988635  0.44022721  0.44045431  0.44068129\n",
      "  0.44090815  0.44102154  0.44124823  0.44158804  0.4419276   0.44215382\n",
      "  0.44271887  0.4428318   0.44362146  0.44384682  0.44395946  0.44407207\n",
      "  0.44440972  0.44452222  0.44463468  0.44497191  0.44542115  0.44553339\n",
      "  0.44586994  0.44609416  0.44665423  0.44676616  0.44687806  0.4472136\n",
      "  0.44777226  0.44799554  0.4482187   0.44833024  0.44888751  0.44899889\n",
      "  0.4494441   0.44977772  0.4501111   0.45022217  0.45033321  0.45055521\n",
      "  0.45110974  0.45144213  0.45166359  0.45199558  0.45221676  0.45243784\n",
      "  0.45265881  0.45276926  0.45332108  0.45343136  0.45387223  0.45442271\n",
      "  0.45464272  0.45497253  0.4553021   0.45607017  0.45628938  0.45639895\n",
      "  0.45672749  0.45716518  0.4577117   0.45814845  0.45825757  0.45880279\n",
      "  0.45912961  0.45934736  0.45978256  0.46        0.46043458  0.46151923\n",
      "  0.46173586  0.4620606   0.46260134  0.46314145  0.46335731  0.46368092\n",
      "  0.46421978  0.464758    0.46529561  0.46540305  0.46583259  0.4669047\n",
      "  0.46743984  0.46797436  0.46850827  0.46904158  0.46957428  0.47\n",
      "  0.47010637  0.47116876  0.47159304  0.47222876  0.47275787  0.47328638\n",
      "  0.47381431  0.47434165  0.47465777  0.4748684   0.47539457  0.47623524\n",
      "  0.4769696   0.47749346  0.47853944  0.47927028  0.47979162  0.48062459\n",
      "  0.48135226  0.48166378  0.48332184  0.48373546  0.48414874  0.48476799\n",
      "  0.48528342  0.48579831  0.48631266  0.48785244  0.48887626  0.48928519\n",
      "  0.48989795  0.49345719  0.49396356  0.49446941  0.49497475  0.49517674\n",
      "  0.49598387  0.49648766  0.49678969  0.49699095  0.49759421  0.49799598\n",
      "  0.49879856  0.498999    0.49949975  0.50049975  0.500999    0.50149776\n",
      "  0.50199602  0.50249378  0.50358713  0.50398413  0.50497525  0.50547008\n",
      "  0.50576674  0.50596443  0.50655701  0.50675438  0.50714889  0.507937\n",
      "  0.5105879   0.51117512  0.51137071  0.51185936  0.51254268  0.51283526\n",
      "  0.51652686  0.51691392  0.51768716  0.5186521   0.51961524  0.52048055\n",
      "  0.5205766   0.52105662  0.52134442  0.52153619  0.52249402  0.52297227\n",
      "  0.52345009  0.52440442  0.52678269  0.52725705  0.52773099  0.52820451\n",
      "  0.5286776   0.52915026  0.53009433  0.53103672  0.53150729  0.53197744\n",
      "  0.53244718  0.5329165   0.53385391  0.53478968  0.53525695  0.53619026\n",
      "  0.53665631  0.53851648  0.53944416  0.5399074   0.5417564   0.54267854\n",
      "  0.54313902  0.54350713  0.54405882  0.54543561  0.54863467  0.54954527\n",
      "  0.55045436  0.55090834  0.55136195  0.55226805  0.55407581  0.55434646\n",
      "  0.55452683  0.55497748  0.55677644  0.55901699  0.56035703  0.56124861\n",
      "  0.56568542  0.56595053  0.56656862  0.56789083  0.56833089  0.57008771\n",
      "  0.57445626  0.57532599  0.58309519  0.58736701  0.59160798  0.59581876\n",
      "  0.60332413  0.60991803  0.63166447  0.6363961 ]\n",
      "ps_car_15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          1.          1.41421356  1.73205081  2.          2.23606798\n",
      "  2.44948974  2.64575131  2.82842712  3.          3.16227766  3.31662479\n",
      "  3.46410162  3.60555128  3.74165739]\n",
      "ps_calc_01\n",
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n",
      "ps_calc_02\n",
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n",
      "ps_calc_03\n",
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n",
      "ps_calc_04\n",
      "[0 1 2 3 4 5]\n",
      "ps_calc_05\n",
      "[0 1 2 3 4 5 6]\n",
      "ps_calc_06\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "ps_calc_07\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "ps_calc_08\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12]\n",
      "ps_calc_09\n",
      "[0 1 2 3 4 5 6 7]\n",
      "ps_calc_10\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25]\n",
      "ps_calc_11\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "ps_calc_12\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "ps_calc_13\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "ps_calc_14\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
      "ps_calc_15_bin\n",
      "[0 1]\n",
      "ps_calc_16_bin\n",
      "[0 1]\n",
      "ps_calc_17_bin\n",
      "[0 1]\n",
      "ps_calc_18_bin\n",
      "[0 1]\n",
      "ps_calc_19_bin\n",
      "[0 1]\n",
      "ps_calc_20_bin\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#Toutes les valeurs possibles de chaque variable\n",
    "for i in range(0,train.shape[1]):\n",
    "    print(train.columns[i])\n",
    "    print(np.unique(train.iloc[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train.filter = matrice contenant les variables catégorielles \n",
    "categorical_features = train.filter(like='cat', axis=1).columns.values.tolist()\n",
    "#categorical_features.remove('ps_car_11_cat') # cette variable catégorie possède beaucoup de catégories, il faut la traiter à part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Les variables catégorielles sont déjà des integers il nous reste plus qu'à utiliser la technique du one-hot encoding\n",
    "#columns = colonnes sélectionnées pour faire la transformation\n",
    "porto_train = pd.get_dummies(train, columns=categorical_features)\n",
    "porto_test = pd.get_dummies(test, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "#Nombre de variables après transformation avec get_dummies: 126 pour train et 125 pour test sans le target\n",
    "print(porto_train.shape[1])\n",
    "print(porto_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On supprime toutes les colonnes contenant -1 dans le nom car elles ne sont pas pertinentes\n",
    "cols = [c for c in porto_train.columns if c[-2:] == '-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nous avons enfin réussi à nous débarrasser des valeurs manquantes de la bonne manière\n",
    "porto_train = porto_train.drop(cols,1)\n",
    "porto_test = porto_test.drop(cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "219\n"
     ]
    }
   ],
   "source": [
    "#Il nous reste 117 et 116 variables pour train et test respectivement\n",
    "print(porto_train.shape[1])\n",
    "print(porto_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ps_reg_03            18.106490\n",
       "ps_car_14             7.160474\n",
       "ps_car_11             0.000840\n",
       "ps_car_12             0.000168\n",
       "ps_car_11_cat_104     0.000000\n",
       "ps_car_04_cat_8       0.000000\n",
       "ps_car_04_cat_6       0.000000\n",
       "ps_car_04_cat_5       0.000000\n",
       "ps_car_04_cat_4       0.000000\n",
       "ps_car_04_cat_3       0.000000\n",
       "ps_car_04_cat_2       0.000000\n",
       "ps_car_04_cat_1       0.000000\n",
       "ps_car_04_cat_0       0.000000\n",
       "ps_car_03_cat_1       0.000000\n",
       "ps_car_03_cat_0       0.000000\n",
       "ps_car_02_cat_1       0.000000\n",
       "ps_car_02_cat_0       0.000000\n",
       "ps_car_01_cat_11      0.000000\n",
       "ps_car_01_cat_10      0.000000\n",
       "ps_car_01_cat_9       0.000000\n",
       "ps_car_01_cat_8       0.000000\n",
       "ps_car_01_cat_7       0.000000\n",
       "ps_car_01_cat_6       0.000000\n",
       "ps_car_01_cat_5       0.000000\n",
       "ps_car_01_cat_4       0.000000\n",
       "ps_car_01_cat_3       0.000000\n",
       "ps_car_01_cat_2       0.000000\n",
       "ps_car_01_cat_1       0.000000\n",
       "ps_car_01_cat_0       0.000000\n",
       "ps_car_04_cat_7       0.000000\n",
       "                       ...    \n",
       "ps_car_10_cat_0       0.000000\n",
       "ps_car_09_cat_4       0.000000\n",
       "ps_car_11_cat_21      0.000000\n",
       "ps_car_11_cat_22      0.000000\n",
       "ps_car_11_cat_23      0.000000\n",
       "ps_car_11_cat_24      0.000000\n",
       "ps_car_11_cat_47      0.000000\n",
       "ps_car_11_cat_46      0.000000\n",
       "ps_car_11_cat_45      0.000000\n",
       "ps_car_11_cat_44      0.000000\n",
       "ps_car_11_cat_43      0.000000\n",
       "ps_car_11_cat_42      0.000000\n",
       "ps_car_11_cat_41      0.000000\n",
       "ps_car_11_cat_40      0.000000\n",
       "ps_car_11_cat_39      0.000000\n",
       "ps_car_11_cat_38      0.000000\n",
       "ps_car_11_cat_37      0.000000\n",
       "ps_car_11_cat_36      0.000000\n",
       "ps_car_11_cat_35      0.000000\n",
       "ps_car_11_cat_34      0.000000\n",
       "ps_car_11_cat_33      0.000000\n",
       "ps_car_11_cat_32      0.000000\n",
       "ps_car_11_cat_31      0.000000\n",
       "ps_car_11_cat_30      0.000000\n",
       "ps_car_11_cat_29      0.000000\n",
       "ps_car_11_cat_28      0.000000\n",
       "ps_car_11_cat_27      0.000000\n",
       "ps_car_11_cat_26      0.000000\n",
       "ps_car_11_cat_25      0.000000\n",
       "id                    0.000000\n",
       "Length: 220, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Il reste à manipuler les valeurs manquantes des variables binaires (bin) et des variables continues\n",
    "#Regardons d'abord le nombre de valeurs manquantes par variable\n",
    "(np.sum(porto_train == -1)/595212*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values=-1, strategy='mean', axis=0, verbose=0)\n",
    "\n",
    "porto_train2 = imp.fit_transform(porto_train)\n",
    "porto_test2 = imp.fit_transform(porto_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert ndarray to pandas dataframe\n",
    "porto_train2 = pd.DataFrame(porto_train2)\n",
    "porto_test2 = pd.DataFrame(porto_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porto_train2.columns = porto_train.columns\n",
    "porto_test2.columns = porto_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = porto_train2.drop('target', axis=1).values\n",
    "y = porto_train2['target'].values\n",
    "test = porto_test2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection : http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = porto_train.drop('target',axis = 1)\n",
    "y = porto_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Method 1: F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "0.ps_car_13(0.120172)\n",
      "1.ps_ind_03(0.100143)\n",
      "2.ps_reg_03(0.087268)\n",
      "3.ps_ind_05_cat_0(0.072961)\n",
      "4.ps_ind_17_bin(0.067239)\n",
      "5.ps_ind_15(0.052933)\n",
      "6.ps_reg_01(0.045780)\n",
      "7.ps_ind_01(0.040057)\n",
      "8.ps_reg_02(0.034335)\n",
      "9.ps_car_07_cat_1(0.031474)\n",
      "10.ps_car_14(0.022890)\n",
      "11.ps_car_09_cat_1(0.018598)\n",
      "12.ps_ind_09_bin(0.018598)\n",
      "13.ps_ind_16_bin(0.018598)\n",
      "14.ps_car_01_cat_7(0.017167)\n",
      "15.ps_car_15(0.015737)\n",
      "16.ps_ind_06_bin(0.015737)\n",
      "17.ps_ind_07_bin(0.014306)\n",
      "18.ps_car_03_cat_1(0.011445)\n",
      "19.ps_ind_05_cat_2(0.011445)\n",
      "20.id(0.011445)\n",
      "21.ps_car_11(0.010014)\n",
      "22.ps_car_12(0.010014)\n",
      "23.ps_ind_05_cat_6(0.010014)\n",
      "24.ps_car_01_cat_6(0.008584)\n",
      "25.ps_car_09_cat_0(0.008584)\n",
      "26.ps_car_04_cat_0(0.008584)\n",
      "27.ps_car_04_cat_2(0.008584)\n",
      "28.ps_ind_04_cat_0(0.007153)\n",
      "29.ps_car_06_cat_9(0.007153)\n",
      "30.ps_car_11_cat(0.007153)\n",
      "31.ps_car_01_cat_9(0.007153)\n",
      "32.ps_ind_02_cat_1(0.007153)\n",
      "33.ps_car_07_cat_0(0.005722)\n",
      "34.ps_ind_02_cat_2(0.005722)\n",
      "35.ps_calc_02(0.005722)\n",
      "36.ps_car_06_cat_15(0.004292)\n",
      "37.ps_ind_04_cat_1(0.004292)\n",
      "38.ps_car_01_cat_10(0.004292)\n",
      "39.ps_car_06_cat_17(0.004292)\n",
      "40.ps_calc_05(0.004292)\n",
      "41.ps_ind_08_bin(0.004292)\n",
      "42.ps_calc_15_bin(0.002861)\n",
      "43.ps_ind_05_cat_3(0.002861)\n",
      "44.ps_calc_01(0.002861)\n",
      "45.ps_ind_02_cat_3(0.001431)\n",
      "46.ps_calc_07(0.001431)\n",
      "47.ps_car_01_cat_0(0.001431)\n",
      "48.ps_car_06_cat_3(0.001431)\n",
      "49.ps_calc_08(0.001431)\n",
      "50.ps_calc_10(0.001431)\n",
      "51.ps_calc_11(0.001431)\n",
      "52.ps_car_10_cat_0(0.001431)\n",
      "53.ps_calc_04(0.001431)\n",
      "54.ps_calc_14(0.001431)\n",
      "55.ps_calc_13(0.001431)\n",
      "56.ps_car_06_cat_6(0.001431)\n",
      "57.ps_ind_12_bin(0.001431)\n",
      "58.ps_car_05_cat_0(0.001431)\n",
      "59.ps_ind_10_bin(0.000000)\n",
      "60.ps_ind_11_bin(0.000000)\n",
      "61.ps_ind_13_bin(0.000000)\n",
      "62.ps_ind_14(0.000000)\n",
      "63.ps_calc_09(0.000000)\n",
      "64.ps_car_08_cat_1(0.000000)\n",
      "65.ps_calc_06(0.000000)\n",
      "66.ps_car_09_cat_2(0.000000)\n",
      "67.ps_car_08_cat_0(0.000000)\n",
      "68.ps_car_06_cat_12(0.000000)\n",
      "69.ps_ind_18_bin(0.000000)\n",
      "70.ps_car_09_cat_3(0.000000)\n",
      "71.ps_car_06_cat_16(0.000000)\n",
      "72.ps_calc_12(0.000000)\n",
      "73.ps_calc_03(0.000000)\n",
      "74.ps_car_09_cat_4(0.000000)\n",
      "75.ps_car_06_cat_14(0.000000)\n",
      "76.ps_car_06_cat_13(0.000000)\n",
      "77.ps_car_06_cat_10(0.000000)\n",
      "78.ps_car_06_cat_11(0.000000)\n",
      "79.ps_car_06_cat_5(0.000000)\n",
      "80.ps_calc_16_bin(0.000000)\n",
      "81.ps_calc_17_bin(0.000000)\n",
      "82.ps_car_02_cat_0(0.000000)\n",
      "83.ps_car_02_cat_1(0.000000)\n",
      "84.ps_car_03_cat_0(0.000000)\n",
      "85.ps_car_04_cat_1(0.000000)\n",
      "86.ps_car_04_cat_3(0.000000)\n",
      "87.ps_car_04_cat_4(0.000000)\n",
      "88.ps_car_04_cat_5(0.000000)\n",
      "89.ps_car_04_cat_6(0.000000)\n",
      "90.ps_car_04_cat_7(0.000000)\n",
      "91.ps_car_04_cat_8(0.000000)\n",
      "92.ps_car_04_cat_9(0.000000)\n",
      "93.ps_car_05_cat_1(0.000000)\n",
      "94.ps_car_06_cat_0(0.000000)\n",
      "95.ps_car_06_cat_1(0.000000)\n",
      "96.ps_car_06_cat_2(0.000000)\n",
      "97.ps_car_01_cat_11(0.000000)\n",
      "98.ps_car_01_cat_8(0.000000)\n",
      "99.ps_car_06_cat_7(0.000000)\n",
      "100.ps_car_06_cat_8(0.000000)\n",
      "101.ps_calc_18_bin(0.000000)\n",
      "102.ps_calc_19_bin(0.000000)\n",
      "103.ps_calc_20_bin(0.000000)\n",
      "104.ps_car_06_cat_4(0.000000)\n",
      "105.ps_ind_02_cat_4(0.000000)\n",
      "106.ps_ind_05_cat_1(0.000000)\n",
      "107.ps_ind_05_cat_4(0.000000)\n",
      "108.ps_car_01_cat_5(0.000000)\n",
      "109.ps_ind_05_cat_5(0.000000)\n",
      "110.ps_car_10_cat_1(0.000000)\n",
      "111.ps_car_01_cat_1(0.000000)\n",
      "112.ps_car_01_cat_2(0.000000)\n",
      "113.ps_car_01_cat_3(0.000000)\n",
      "114.ps_car_01_cat_4(0.000000)\n",
      "115.ps_car_10_cat_2(0.000000)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X, y)\n",
    "# feature importance\n",
    "importances = xgb.feature_importances_ #calcule les F-score (feature scores)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "colonnesX = X.columns\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "#    print(\"%d. feature %d (%f)\" % (f, indices[f], importances[indices[f]]))\n",
    "    print(\"%d.%s(%f)\" % (f,colonnesX[indices[f]],importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all true positives. The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzwAAAIwCAYAAABDQ24hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRkZGRkZGxmjx/z+GtXznR6L9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb4dca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature importances for xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X, y)\n",
    "def feat_imp(df, model, n_features):\n",
    "\n",
    "    d = dict(zip(df.columns, model.feature_importances_))\n",
    "    ss = sorted(d, key=d.get, reverse=True)\n",
    "    top_names = ss[0:n_features]\n",
    "\n",
    "    pyplot.figure(figsize=(14,8))\n",
    "    pyplot.title(\"Feature importances\")\n",
    "    pyplot.bar(range(n_features), [d[i] for i in top_names], color=\"r\", align=\"center\")\n",
    "    pyplot.xlim(-1, n_features)\n",
    "    pyplot.xticks(range(n_features), top_names, rotation='vertical')\n",
    "    pyplot.show()\n",
    "\n",
    "feat_imp(X, xgb, 58)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Method 2: SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation. For classification: chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_selection\n",
    "\n",
    "def KBest(k):\n",
    "    select = sklearn.feature_selection.SelectKBest(k=k)\n",
    "    selected_features = select.fit(X_train,y_train)\n",
    "    indices_selected = selected_features.get_support(indices = True)\n",
    "    colnames_selected = [X.columns[i] for i in indices_selected]\n",
    "    return colnames_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def find_model_perf(X_train,y_train,X_test,y_test, model):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "#    auc = roc_auc_score(y_test,y_pred_prob)\n",
    "#    return auc\n",
    "\n",
    "    gini = normalized_gini(y_test,y_pred_prob)\n",
    "    return gini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "[0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747, 0.96355368383324747]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "auc = []\n",
    "i = 50\n",
    "for k in range(50,X_train.shape[1]):\n",
    "    X_train_selected = X_train[KBest(k)]\n",
    "    X_test_selected = X_test[KBest(k)]\n",
    "    auc.append(find_model_perf(X_train_selected, y_train, X_test_selected, y_test, model))\n",
    "    print(i)\n",
    "    i = i + 1\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ps_ind_01', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15', 'ps_ind_05_cat_0', 'ps_ind_05_cat_2', 'ps_ind_05_cat_6', 'ps_car_01_cat_7', 'ps_car_01_cat_9', 'ps_car_01_cat_11', 'ps_car_02_cat_0', 'ps_car_02_cat_1', 'ps_car_03_cat_1', 'ps_car_04_cat_0', 'ps_car_04_cat_9', 'ps_car_06_cat_9', 'ps_car_06_cat_15', 'ps_car_07_cat_0', 'ps_car_07_cat_1', 'ps_car_08_cat_0', 'ps_car_08_cat_1', 'ps_car_09_cat_1']\n"
     ]
    }
   ],
   "source": [
    "best_features = KBest(k = 30)\n",
    "print(best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Analyse de la variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  1.843582e+11\n",
       "ps_car_11_cat       1.089764e+03\n",
       "ps_ind_15           1.257971e+01\n",
       "ps_calc_10          8.439194e+00\n",
       "ps_calc_14          7.546598e+00\n",
       "ps_ind_03           7.285226e+00\n",
       "ps_calc_11          5.437532e+00\n",
       "ps_ind_01           3.941089e+00\n",
       "ps_calc_13          2.875740e+00\n",
       "ps_calc_08          2.130212e+00\n",
       "ps_calc_07          1.999525e+00\n",
       "ps_calc_06          1.780735e+00\n",
       "ps_calc_09          1.555248e+00\n",
       "ps_calc_12          1.447914e+00\n",
       "ps_calc_05          1.289045e+00\n",
       "ps_calc_04          1.248740e+00\n",
       "ps_car_11           6.934660e-01\n",
       "ps_reg_03           6.301632e-01\n",
       "ps_car_15           5.359944e-01\n",
       "ps_calc_17_bin      2.470762e-01\n",
       "ps_ind_04_cat_0     2.431947e-01\n",
       "ps_ind_04_cat_1     2.431713e-01\n",
       "ps_car_09_cat_2     2.413091e-01\n",
       "ps_ind_06_bin       2.386925e-01\n",
       "ps_calc_16_bin      2.335200e-01\n",
       "ps_calc_19_bin      2.272489e-01\n",
       "ps_car_01_cat_11    2.271467e-01\n",
       "ps_ind_16_bin       2.240050e-01\n",
       "ps_car_09_cat_0     2.202245e-01\n",
       "ps_car_01_cat_7     2.101579e-01\n",
       "                        ...     \n",
       "ps_ind_14           1.630309e-02\n",
       "ps_ind_05_cat_1     1.376884e-02\n",
       "ps_ind_05_cat_3     1.347945e-02\n",
       "ps_car_01_cat_3     1.105472e-02\n",
       "ps_car_06_cat_13    1.041606e-02\n",
       "ps_car_01_cat_0     9.861236e-03\n",
       "ps_ind_12_bin       9.409383e-03\n",
       "ps_car_10_cat_1     8.428911e-03\n",
       "ps_car_06_cat_17    8.195284e-03\n",
       "ps_car_10_cat_0     8.143351e-03\n",
       "ps_car_06_cat_16    7.569362e-03\n",
       "ps_ind_05_cat_2     6.871630e-03\n",
       "ps_car_09_cat_4     4.667816e-03\n",
       "ps_car_06_cat_12    4.134945e-03\n",
       "ps_car_01_cat_2     3.506177e-03\n",
       "ps_car_12           3.405616e-03\n",
       "ps_ind_05_cat_5     2.690443e-03\n",
       "ps_car_06_cat_2     2.671346e-03\n",
       "ps_car_04_cat_6     2.671346e-03\n",
       "ps_car_06_cat_8     2.363294e-03\n",
       "ps_car_06_cat_5     2.348962e-03\n",
       "ps_car_01_cat_1     2.262957e-03\n",
       "ps_ind_11_bin       1.713132e-03\n",
       "ps_car_04_cat_3     1.078882e-03\n",
       "ps_ind_13_bin       9.351658e-04\n",
       "ps_car_04_cat_5     8.992302e-04\n",
       "ps_car_04_cat_4     3.814715e-04\n",
       "ps_ind_10_bin       3.718783e-04\n",
       "ps_car_10_cat_2     2.903287e-04\n",
       "ps_car_04_cat_7     2.375542e-04\n",
       "Length: 116, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(X_train).sort_values(ascending = False)\n",
    "#variance très grande pour ps_car_11_cat car bcp de valeurs comme on l'a vu précedemment\n",
    "#on fixe un seuil pour la variance? on supprime les features avec une variance trop grande ou trop petite ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des modèles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def find_model_perf(X_train,y_train,X_test,y_test, model):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "#    auc = roc_auc_score(y_test,y_pred_prob)\n",
    "#    return auc\n",
    "\n",
    "#    gini = normalized_gini(y_test,y_pred_prob) #gini normalisé classique (voir tout en haut)\n",
    "#    return gini\n",
    "\n",
    "    gini2 = normalized_gini(y_pred_prob,y_test) #gini normalisé de BINH (voir tout en haut)\n",
    "    return gini2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6357625655762682, 0.49852723458999315, 0.53789798080063633, 0.53539510747404917, 0.53423516679108118, 0.50583652578744753]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.svm import SVC (trop long)\n",
    "\n",
    "scores = []\n",
    "models = [XGBClassifier(), LogisticRegression(), GaussianNB(), RandomForestClassifier(), ExtraTreesClassifier(), DecisionTreeClassifier()]\n",
    "for i in models:\n",
    "    scores.append(find_model_perf(X_train, y_train, X_test, y_test, i))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505214502174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, KNeighborsClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec get_dummies pour toutes les variables catégorielles, le meilleur modèle est XGBClassifier pour la métrique roc_auc_score.\n",
    "[XGBClassifier(), LogisticRegression(), GaussianNB(), RandomForestClassifier(), ExtraTreesClassifier(), DecisionTreeClassifier(), KNeighborsClassifier()]\n",
    "[0.6357625655762682, 0.49852723458999315, 0.53789798080063633, 0.53539510747404917, 0.53423516679108118, 0.50583652578744753, 0.505214502174]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GINI NORMALISE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.269367659745\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, XGBClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00206741125065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, LogisticRegression())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0807680281279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, GaussianNB())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0661300921408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, RandomForestClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0799807706866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, ExtraTreesClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0121590892106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, DecisionTreeClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00270552698655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, KNeighborsClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec get_dummies pour toutes les variables catégorielles, le meilleur modèle est XGBClassifier pour la métrique gini normalisé\n",
    "[XGBClassifier(), LogisticRegression(), GaussianNB(), RandomForestClassifier(), ExtraTreesClassifier(), DecisionTreeClassifier(), KNeighborsClassifier()]\n",
    "[0.269367659745, -0.00206741125065, 0.0807680281279, 0.0661300921408, 0.0799807706866, 0.0121590892106, 0.00270552698655]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GINI NORMALISE 2 (BINH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richard\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.262092598802\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, XGBClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00939402699866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, LogisticRegression())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0927778773961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, GaussianNB())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0583219139012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, RandomForestClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048320981971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, ExtraTreesClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00612834520023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, DecisionTreeClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00756881473304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "score = find_model_perf(X_train, y_train, X_test, y_test, KNeighborsClassifier())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec get_dummies pour toutes les variables catégorielles, le meilleur modèle est XGBClassifier pour la métrique gini normalisé de BINH[XGBClassifier(), LogisticRegression(), GaussianNB(), RandomForestClassifier(), ExtraTreesClassifier(), DecisionTreeClassifier(), KNeighborsClassifier()] [0.262092598802, 0.00939402699866, 0.0927778773961\n",
    ", 0.0583219139012, 0.048320981971, 0.00612834520023, -0.00756881473304]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche des paramètres optimaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "[CV] max_depth=2 .....................................................\n",
      "[CV] ............................................ max_depth=2 - 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] max_depth=2 .....................................................\n",
      "[CV] ............................................ max_depth=2 - 1.2min\n",
      "[CV] max_depth=2 .....................................................\n",
      "[CV] ............................................ max_depth=2 - 1.1min\n",
      "[CV] max_depth=2 .....................................................\n",
      "[CV] ............................................ max_depth=2 -  59.3s\n",
      "[CV] max_depth=2 .....................................................\n",
      "[CV] ............................................ max_depth=2 - 1.0min\n",
      "[CV] max_depth=3 .....................................................\n",
      "[CV] ............................................ max_depth=3 - 1.4min\n",
      "[CV] max_depth=3 .....................................................\n",
      "[CV] ............................................ max_depth=3 - 1.4min\n",
      "[CV] max_depth=3 .....................................................\n",
      "[CV] ............................................ max_depth=3 - 1.4min\n",
      "[CV] max_depth=3 .....................................................\n",
      "[CV] ............................................ max_depth=3 - 1.4min\n",
      "[CV] max_depth=3 .....................................................\n",
      "[CV] ............................................ max_depth=3 - 1.4min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................................ max_depth=4 - 1.8min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................................ max_depth=4 - 1.8min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................................ max_depth=4 - 1.8min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................................ max_depth=4 - 1.8min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................................ max_depth=4 - 1.8min\n",
      "[CV] max_depth=5 .....................................................\n",
      "[CV] ............................................ max_depth=5 - 2.2min\n",
      "[CV] max_depth=5 .....................................................\n",
      "[CV] ............................................ max_depth=5 - 2.2min\n",
      "[CV] max_depth=5 .....................................................\n",
      "[CV] ............................................ max_depth=5 - 2.2min\n",
      "[CV] max_depth=5 .....................................................\n",
      "[CV] ............................................ max_depth=5 - 2.2min\n",
      "[CV] max_depth=5 .....................................................\n",
      "[CV] ............................................ max_depth=5 - 2.2min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................................ max_depth=6 - 2.6min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................................ max_depth=6 - 2.6min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................................ max_depth=6 - 2.8min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................................ max_depth=6 - 3.5min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................................ max_depth=6 - 3.4min\n",
      "[CV] max_depth=7 .....................................................\n",
      "[CV] ............................................ max_depth=7 - 3.6min\n",
      "[CV] max_depth=7 .....................................................\n",
      "[CV] ............................................ max_depth=7 - 3.7min\n",
      "[CV] max_depth=7 .....................................................\n",
      "[CV] ............................................ max_depth=7 - 3.4min\n",
      "[CV] max_depth=7 .....................................................\n",
      "[CV] ............................................ max_depth=7 - 3.5min\n",
      "[CV] max_depth=7 .....................................................\n",
      "[CV] ............................................ max_depth=7 - 3.7min\n",
      "[CV] max_depth=8 .....................................................\n",
      "[CV] ............................................ max_depth=8 - 4.3min\n",
      "[CV] max_depth=8 .....................................................\n",
      "[CV] ............................................ max_depth=8 - 4.3min\n",
      "[CV] max_depth=8 .....................................................\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "params_grid={\n",
    "    'max_depth': [2,3,4,5,6,7,8],\n",
    "#    'subsample': [0.4,0.5,0.6,0.7,0.8,0.9,1.0], \n",
    "#    'colsample_bytree': [0.5,0.6,0.7,0.8], \n",
    "#    'min_child_weight'=1\n",
    "#    'gamma' : [0,0.5,1],\n",
    "#    'n_estimators' : [200],\n",
    "#    'learning_rate' : [0.20,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3],\n",
    "#    'subsample' : [0.5],\n",
    "#    'colsample_bytree' : [0.5],\n",
    "#    'learning_rate' : [0.05,0.06,0.07],\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    'objective' : 'binary:logistic'\n",
    "}\n",
    "\n",
    "seed = 342\n",
    "np.random.seed(seed)\n",
    "best_grid = GridSearchCV(XGBClassifier(**params_fixed, seed = seed),\n",
    "                  params_grid,\n",
    "                  scoring='roc_auc',\n",
    "                  cv = 5,\n",
    "                  verbose=2)\n",
    "best_grid.fit(X_train, y_train)\n",
    "accuracy = best_grid.best_score_\n",
    "print(\"Best gini obtained: {0}\".format(accuracy))\n",
    "print(\"Parameters: \")\n",
    "for key,value in best_grid.best_params_.items():\n",
    "    print(\"\\t{}:{}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert train & test to numpy arrays\n",
    "X = porto_train.drop('target', axis=1).values\n",
    "y = porto_train['target'].values\n",
    "test = porto_test.values\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = porto_test['id'].to_frame()\n",
    "submission['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "nrounds=500\n",
    "params = {\n",
    "    'eta': 0.07,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree':0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'silent': True,\n",
    "    'n_jobs':1 # use all CPU cores\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=kfold, shuffle=True, random_state=322)\n",
    "\n",
    "for i, (train_index, cv_index) in enumerate(kf.split(X)):\n",
    "\n",
    "    print(' xgb kfold: {} of {} : '.format(i+1, kfold))\n",
    "    X_train, X_eval = X[train_index,:], X[cv_index,:]\n",
    "    y_train, y_eval = y[train_index], y[cv_index]\n",
    "\n",
    "    d_train = xgb.DMatrix(X_train, y_train) \n",
    "    d_valid = xgb.DMatrix(X_eval, y_eval)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "    xgb_model = xgb.train(params,\n",
    "                          d_train,\n",
    "                          nrounds,\n",
    "                          watchlist,\n",
    "                          early_stopping_rounds=100,\n",
    "                          feval=gini_xgb,\n",
    "                          maximize=True,\n",
    "                          verbose_eval=50)\n",
    "    # Updating prediction by taking the average\n",
    "    submission['target'] += xgb_model.predict(xgb.DMatrix(test)) / kfold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
